{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №4 - Градиентный бустинг\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 21 декабря 2020, 08:30   \n",
    "**Штраф за опоздание:** -2 балла после 08:30 21 декабря, -4 балла после 08:30 28 декабря, -6 баллов после 08:30 04 янва, -8 баллов после 08:30 11 января.\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла Присылать ДЗ необходимо в виде ссылки на свой github репозиторий на почту ml1.sphere@mail.ru с указанием темы в следующем формате:\n",
    "[ML0220, Задание 4] Фамилия Имя. \n",
    "\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Считаем производные для функций потерь (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем реализовать градиентный бустинг для 3 функций потерь:\n",
    "\n",
    "1) MSE  $L(a(x_i), y_i) = (y_i - a(x_i)) ^ 2$\n",
    "\n",
    "2) Экспоненциальная  $L(a(x_i), y_i) = exp( -a(x_i) y_i), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "3) Логистическая  $L(a(x_i), y_i) = \\log (1 + exp( -a(x_i) y_i)), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "где $a(x_i)$ предсказание бустинга на итом объекте. \n",
    "\n",
    "Для каждой функции потерь напишите таргет, на который будет настраиваться каждое дерево в бустинге. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваше решение тут\n",
    "\n",
    "для k-ого алгоритма в бустинге таргет:\n",
    "\n",
    "mse:\n",
    "$$y_{ki} = 2(y_i - F_{k-1}(x_i))$$\n",
    "exp:\n",
    "$$y_{ki} = y_iexp(-F_{k-1}(x_i)y_i)$$\n",
    "log:\n",
    "$$ y_{ki} = \\frac{y_iexp(-F_{k-1}(x_i)y_i)}{1+exp(-F_{k-1}(x_i)y_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Реализуем градиентный бустинг (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс градиентного бустинга для классификации. Ваша реализация бустинга должна работать по точности не более чем на 5 процентов хуже чем GradientBoostingClassifier из sklearn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детали реализации:\n",
    "\n",
    "-- должно поддерживаться 3 функции потерь\n",
    "\n",
    "-- сами базовые алгоритмы(деревья, линейные модели и тп) реализовать не надо, просто возьмите готовые из sklearn\n",
    "\n",
    "-- в качестве функции потерь для построения одного дерева используйте MSE\n",
    "\n",
    "-- шаг в бустинге можно не подбирать, можно брать константный\n",
    "\n",
    "-- можно брать разные модели в качестве инициализации бустинга\n",
    "\n",
    "-- должны поддерживаться следующие параметры:\n",
    "\n",
    "а) число итераций\n",
    "б) размер шага\n",
    "в) процент случайных фичей при построении одного дерева\n",
    "д) процент случайных объектов при построении одного дерева\n",
    "е) параметры базового алгоритма (передавайте через **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingClassifier:\n",
    "    \n",
    "    def new_y(self, y, F):\n",
    "        if self.loss == 'mse':\n",
    "            return 2 * (y - F)\n",
    "        if self.loss == 'exp':\n",
    "            return y * np.exp(-F * y)\n",
    "        if self.loss == 'log':\n",
    "            return y * np.exp(-F * y) / (1 + np.exp(-F * y))\n",
    "\n",
    "    def __init__(self, loss='mse', learning_rate=0.2, n_estimators=6, colsample=0.5, subsample=0.5):\n",
    "        \"\"\"\n",
    "        loss -- один из 3 лоссов:\n",
    "        learning_rate -- шаг бустинга\n",
    "        n_estimators -- число итераций\n",
    "        colsample -- процент рандомных признаков при обучнеии одного алгоритма\n",
    "        colsample -- процент рандомных объектов при обучнеии одного алгоритма\n",
    "        \"\"\"\n",
    "        # Ваш код здесь\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.colsample = colsample\n",
    "        self.subsample = subsample\n",
    "    \n",
    "    def fit(self, X, y, base_model=DecisionTreeRegressor, init_model=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        X -- объекты для обучения:\n",
    "        y -- таргеты для обучения\n",
    "        base_model -- класс базовых моделей, например sklearn.tree.DecisionTreeRegressor\n",
    "        init_model -- класс для первой модели, если None то берем константу (только для посл задания)\n",
    "        args, kwargs -- параметры  базовых моделей\n",
    "        \"\"\"\n",
    "        # Ваш код здесь\n",
    "        self.models = []\n",
    "        self.cols = []\n",
    "        \n",
    "        if init_model is None:\n",
    "            F = np.zeros(len(y))\n",
    "        else:\n",
    "            model = init_model()\n",
    "            model.fit(X, y)\n",
    "            self.models.append(model)\n",
    "            self.cols.append(list(range(X.shape[1])))\n",
    "            F = np.asarray(model.predict(X))\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            #colsample\n",
    "            cols = list(range(X.shape[1]))\n",
    "            random.shuffle(cols)\n",
    "            cols = cols[:round(X.shape[1] * self.colsample)]\n",
    "            X_i_all = X[:, cols] \n",
    "            \n",
    "            #subsample\n",
    "            subs = list(range(X.shape[0]))\n",
    "            random.shuffle(subs)\n",
    "            subs = subs[:round(X.shape[0] * self.subsample)]\n",
    "            X_i = X_i_all[subs, :] \n",
    "            y_i = y[subs]\n",
    "            F_i = F[subs] \n",
    "            \n",
    "            #fit\n",
    "            model = base_model(*args, **kwargs)\n",
    "            model.fit(X_i, self.new_y(y_i, F_i))\n",
    "            self.models.append(model)\n",
    "            self.cols.append(cols)\n",
    "            F = F + self.learning_rate * np.asarray(model.predict(X_i_all))\n",
    "            \n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Ваш код здесь\n",
    "        F = np.zeros(len(X))\n",
    "        for i in range(len(self.models)):\n",
    "            F = F + self.learning_rate * np.asarray(self.models[i].predict(X[:, self.cols[i]]))\n",
    "        return np.round(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyGradientBoostingClassifier()\n",
    "clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8888888888888888\n",
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "my_clf.fit(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_pred=clf.predict(X_test), y_true=y_test))\n",
    "print(accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9472222222222222, 0.9238888888888888)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Как-то сильно скор отличается в зависимости от train_test_split, захотелось посмотреть в среднем\n",
    "accuracy_sklearn = []\n",
    "accuracy_my = []\n",
    "wine = load_wine()\n",
    "for i in tqdm(range(100)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)\n",
    "    my_clf.fit(X_train, y_train)\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy_sklearn.append(accuracy_score(y_pred=clf.predict(X_test), y_true=y_test))\n",
    "    accuracy_my.append(accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))\n",
    "np.mean(accuracy_sklearn), np.mean(accuracy_my)\n",
    "# Как раз меньше пяти процентов разница"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбираем параметры (2 балла)\n",
    "\n",
    "Давайте попробуем применить Ваш бустинг для предсказаний цены домов в Калифорнии. Чтобы можно было попробовтаь разные функции потерь, переведем по порогу таргет в 2 класса: дорогие и дешевые дома."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании нужно\n",
    "\n",
    "1) Построить график точности в зависимости от числа итераций на валидации.\n",
    "\n",
    "2) Подобрать оптимальные параметры Вашего бустинга на валидации. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "X, y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8) (20640,)\n"
     ]
    }
   ],
   "source": [
    "# Превращаем регрессию в классификацию\n",
    "y = (y > 2.0).astype(int)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15bb8faa730>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = []\n",
    "for n in range(1, 10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "    my_clf = MyGradientBoostingClassifier(n_estimators=n)\n",
    "    my_clf.fit(X_train, y_train)\n",
    "    acc.append(accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))\n",
    "plt.plot(list(range(1, 10)), acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [03:22<00:00, 33.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_accuracy = 0.8842054263565892 with loss ' exp ', colsample = 0.7066666666666667 , subsample = 0.67 , n_estimators = 5 and learning rate = 0.326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_loss = \"\"\n",
    "best_lr = 0\n",
    "best_c = 0\n",
    "best_s = 0\n",
    "best_n = 0\n",
    "for alpha in tqdm(np.linspace(0.01, 0.8, 6)):\n",
    "    for n in range(2, 6):\n",
    "        for c in np.linspace(0.12, 1, 4):\n",
    "            for s in np.linspace(0.01, 1, 4):\n",
    "                for l in ['mse', 'exp', 'log']:\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "                    my_clf = MyGradientBoostingClassifier(loss = l, learning_rate=alpha, \n",
    "                                                          n_estimators=n, colsample=c, subsample=s)\n",
    "                    my_clf.fit(X_train, y_train)\n",
    "                    acc = accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test)\n",
    "                    if acc > best_acc:\n",
    "                        best_acc = acc\n",
    "                        best_loss = l\n",
    "                        best_lr = alpha\n",
    "                        best_c = c\n",
    "                        best_s = s\n",
    "                        best_n = n\n",
    "print(\"best_accuracy =\", best_acc, \"with loss '\", best_loss, \"', colsample =\", \n",
    "      best_c, \", subsample =\", best_s, \", n_estimators =\", best_n, \"and learning rate =\", best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BooBag BagBoo (1 балл)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем объединить бустинг и бэгинг. Давайте\n",
    "\n",
    "1) в качестве базовой модели брать не дерево решений, а случайный лес (из sklearn)\n",
    "\n",
    "2) обучать N бустингов на бустрапированной выборке, а затем предикт усреднять"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте обе этих стратегии на данных из прошлого задания. Получилось ли улучшить качество? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бустинг просто на деревьях: 0.872093023255814\n",
      "бустинг на лесах из N деревьев с теми же параметрами бустинга: 0.876937984496124\n"
     ]
    }
   ],
   "source": [
    "N = 20 # колво бустингов / колво деревьев в каждом лесе\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "my_clf = MyGradientBoostingClassifier(loss = best_loss, learning_rate=best_lr, \n",
    "                                        n_estimators=best_n, colsample=best_c, subsample=best_s)\n",
    "my_clf.fit(X_train, y_train, DecisionTreeRegressor)\n",
    "print(\"бустинг просто на деревьях:\", accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "my_clf = MyGradientBoostingClassifier(loss = best_loss, learning_rate=best_lr, \n",
    "                                        n_estimators=n, colsample=c, subsample=s)\n",
    "my_clf.fit(X_train, y_train, RandomForestRegressor, n_estimators=N)\n",
    "print(\"бустинг на лесах из N деревьев с теми же параметрами бустинга:\", accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "усреднение по N бустингам с теми же параметрами: 0.9118217054263565\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "ans = np.zeros(len(y_test))\n",
    "for i in range(N):\n",
    "    indices = np.random.randint(0, len(y_train), (1, len(y_train)))[0]\n",
    "    X_b = X_train[indices, :]\n",
    "    y_b = y_train[indices]\n",
    "    my_clf = MyGradientBoostingClassifier(loss = best_loss, learning_rate=best_lr, \n",
    "                                        n_estimators=best_n, colsample=best_c, subsample=best_s)\n",
    "    my_clf.fit(X_b, y_b, DecisionTreeRegressor)\n",
    "    ans += my_clf.predict(X_test)\n",
    "ans = ans / N\n",
    "ans = np.round(ans)\n",
    "print(\"усреднение по N бустингам с теми же параметрами:\", accuracy_score(y_pred=ans, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Качество улучшилось в обоих способах. Видимо, variance был довольно велик \n",
    "# в бустинге из-за переобучения, и усреднение на каждом шаге \n",
    "# по N деревьям / усреднение N бустингов этот variance уменьшило"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Умная инициализация (1 балл)\n",
    "\n",
    "Попробуйте брать в качестве инициализации бустинга не константу, а какой-то алгоритм и уже от его предикта стартовать итерации бустинга. Попробуйте разные модели из sklearn: линейные модели, рандом форест, svm..\n",
    "\n",
    "Получилось ли улучшить качество? Почему?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init is Forest: 0.8919573643410853\n",
      "init is SVC: 0.8771802325581395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sasha\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init is LogisticRegression: 0.8364825581395349\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "my_clf = MyGradientBoostingClassifier(loss = best_loss, learning_rate=best_lr, \n",
    "                                    n_estimators=best_n, colsample=best_c, subsample=best_s)\n",
    "my_clf.fit(X_train, y_train, DecisionTreeRegressor, RandomForestClassifier)\n",
    "print(\"init is Forest:\", accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "my_clf = MyGradientBoostingClassifier(loss = best_loss, learning_rate=best_lr, \n",
    "                                    n_estimators=best_n, colsample=best_c, subsample=best_s)\n",
    "my_clf.fit(X_train, y_train, DecisionTreeRegressor, SVC)\n",
    "print(\"init is SVC:\", accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "my_clf = MyGradientBoostingClassifier(loss = best_loss, learning_rate=best_lr, \n",
    "                                    n_estimators=best_n, colsample=best_c, subsample=best_s)\n",
    "my_clf.fit(X_train, y_train, DecisionTreeRegressor, LogisticRegression)\n",
    "print(\"init is LogisticRegression:\", accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest с большим количеством деревьев в качестве init_model даёт прирост в точности,\n",
    "# так как лес сам по себе хорошо разделяет классы, а потом деревья доисправляют ошибки.\n",
    "\n",
    "# SVC и LogisticRegression дают незначительное изменение в точности. Я думаю, это связано с тем,\n",
    "# что данные не очень хорошо линейно разделимы, и при инициализации получается достаточная ошибка, \n",
    "# которую деревья не успевают полностью исправить за колво итераций n_estimators и шагом learning_rate, \n",
    "# которые подбирались для константной инициализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения  ансамблей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ваш ответ здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Здесь Вы можете оставить отзыв о этой домашней работе или о всем курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ВАШ ОТЗЫВ ЗДЕСЬ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
