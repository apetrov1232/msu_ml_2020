{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №4 - Градиентный бустинг\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 21 декабря 2020, 08:30   \n",
    "**Штраф за опоздание:** -2 балла после 08:30 21 декабря, -4 балла после 08:30 28 декабря, -6 баллов после 08:30 04 янва, -8 баллов после 08:30 11 января.\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла Присылать ДЗ необходимо в виде ссылки на свой github репозиторий на почту ml1.sphere@mail.ru с указанием темы в следующем формате:\n",
    "[ML0220, Задание 4] Фамилия Имя. \n",
    "\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Считаем производные для функций потерь (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем реализовать градиентный бустинг для 3 функций потерь:\n",
    "\n",
    "1) MSE  $L(a(x_i), y_i) = (y_i - a(x_i)) ^ 2$\n",
    "\n",
    "2) Экспоненциальная  $L(a(x_i), y_i) = exp( -a(x_i) y_i), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "3) Логистическая  $L(a(x_i), y_i) = \\log (1 + exp( -a(x_i) y_i)), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "где $a(x_i)$ предсказание бустинга на итом объекте. \n",
    "\n",
    "Для каждой функции потерь напишите таргет, на который будет настраиваться каждое дерево в бустинге. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваше решение тут\n",
    "\n",
    "для k-ого алгоритма в бустинге таргет:\n",
    "\n",
    "mse:\n",
    "$$y_{ki} = 2(y_i - F_{k-1}(x_i))$$\n",
    "exp:\n",
    "$$y_{ki} = y_iexp(-F_{k-1}(x_i)y_i)$$\n",
    "log:\n",
    "$$ y_{ki} = \\frac{y_iexp(-F_{k-1}(x_i)y_i)}{1+exp(-F_{k-1}(x_i)y_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Реализуем градиентный бустинг (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс градиентного бустинга для классификации. Ваша реализация бустинга должна работать по точности не более чем на 5 процентов хуже чем GradientBoostingClassifier из sklearn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детали реализации:\n",
    "\n",
    "-- должно поддерживаться 3 функции потерь\n",
    "\n",
    "-- сами базовые алгоритмы(деревья, линейные модели и тп) реализовать не надо, просто возьмите готовые из sklearn\n",
    "\n",
    "-- в качестве функции потерь для построения одного дерева используйте MSE\n",
    "\n",
    "-- шаг в бустинге можно не подбирать, можно брать константный\n",
    "\n",
    "-- можно брать разные модели в качестве инициализации бустинга\n",
    "\n",
    "-- должны поддерживаться следующие параметры:\n",
    "\n",
    "а) число итераций\n",
    "б) размер шага\n",
    "в) процент случайных фичей при построении одного дерева\n",
    "д) процент случайных объектов при построении одного дерева\n",
    "е) параметры базового алгоритма (передавайте через **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingClassifier:\n",
    "    \n",
    "    def new_y(self, y, F):\n",
    "        if self.loss == 'mse':\n",
    "            return 2 * (y - F)\n",
    "        if self.loss == 'exp':\n",
    "            return y * np.exp(-F * y)\n",
    "        if self.loss == 'log':\n",
    "            return y * np.exp(-F * y) / (1 + np.exp(-F * y))\n",
    "\n",
    "    def __init__(self, loss='mse', learning_rate=0.2, n_estimators=6, colsample=0.5, subsample=0.5):\n",
    "        \"\"\"\n",
    "        loss -- один из 3 лоссов:\n",
    "        learning_rate -- шаг бустинга\n",
    "        n_estimators -- число итераций\n",
    "        colsample -- процент рандомных признаков при обучнеии одного алгоритма\n",
    "        colsample -- процент рандомных объектов при обучнеии одного алгоритма\n",
    "        \"\"\"\n",
    "        # Ваш код здесь\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.colsample = colsample\n",
    "        self.subsample = subsample\n",
    "    \n",
    "    def fit(self, X, y, base_model=DecisionTreeRegressor, init_model=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        X -- объекты для обучения:\n",
    "        y -- таргеты для обучения\n",
    "        base_model -- класс базовых моделей, например sklearn.tree.DecisionTreeRegressor\n",
    "        init_model -- класс для первой модели, если None то берем константу (только для посл задания)\n",
    "        args, kwargs -- параметры  базовых моделей\n",
    "        \"\"\"\n",
    "        # Ваш код здесь\n",
    "        self.models = []\n",
    "        self.cols = []\n",
    "        \n",
    "        if init_model is None:\n",
    "            F = np.zeros(len(y))\n",
    "        else:\n",
    "            model = init_model()\n",
    "            model.fit(X, y)\n",
    "            self.models.append(model)\n",
    "            self.cols.append(list(range(X.shape[1])))\n",
    "            F = np.asarray(model.predict(X))\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            #colsample\n",
    "            cols = list(range(X.shape[1]))\n",
    "            random.shuffle(cols)\n",
    "            cols = cols[:round(X.shape[1] * self.colsample)]\n",
    "            X_i_all = X[:, cols] \n",
    "            \n",
    "            #subsample\n",
    "            subs = list(range(X.shape[0]))\n",
    "            random.shuffle(subs)\n",
    "            subs = subs[:round(X.shape[0] * self.subsample)]\n",
    "            X_i = X_i_all[subs, :] \n",
    "            y_i = y[subs]\n",
    "            F_i = F[subs] \n",
    "            \n",
    "            #fit\n",
    "            model = base_model(*args, **kwargs)\n",
    "            model.fit(X_i, self.new_y(y_i, F_i))\n",
    "            self.models.append(model)\n",
    "            self.cols.append(cols)\n",
    "            F = F + self.learning_rate * np.asarray(model.predict(X_i_all))\n",
    "            \n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Ваш код здесь\n",
    "        F = np.zeros(len(X))\n",
    "        for i in range(len(self.models)):\n",
    "            F = F + self.learning_rate * np.asarray(self.models[i].predict(X[:, self.cols[i]]))\n",
    "        return np.round(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyGradientBoostingClassifier()\n",
    "clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n",
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "my_clf.fit(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_pred=clf.predict(X_test), y_true=y_test))\n",
    "print(accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9483333333333333, 0.9144444444444444)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Как-то сильно скор отличается в зависимости от train_test_split, захотелось посмотреть в среднем\n",
    "accuracy_sklearn = []\n",
    "accuracy_my = []\n",
    "wine = load_wine()\n",
    "for i in tqdm(range(100)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)\n",
    "    my_clf.fit(X_train, y_train)\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy_sklearn.append(accuracy_score(y_pred=clf.predict(X_test), y_true=y_test))\n",
    "    accuracy_my.append(accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))\n",
    "np.mean(accuracy_sklearn), np.mean(accuracy_my)\n",
    "# Как раз меньше пяти процентов разница"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбираем параметры (2 балла)\n",
    "\n",
    "Давайте попробуем применить Ваш бустинг для предсказаний цены домов в Калифорнии. Чтобы можно было попробовтаь разные функции потерь, переведем по порогу таргет в 2 класса: дорогие и дешевые дома."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании нужно\n",
    "\n",
    "1) Построить график точности в зависимости от числа итераций на валидации.\n",
    "\n",
    "2) Подобрать оптимальные параметры Вашего бустинга на валидации. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "X, y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8) (20640,)\n"
     ]
    }
   ],
   "source": [
    "# Превращаем регрессию в классификацию\n",
    "y = (y > 2.0).astype(int)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1419e010940>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = []\n",
    "for n in range(1, 10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "    my_clf = MyGradientBoostingClassifier(n_estimators=n)\n",
    "    my_clf.fit(X_train, y_train)\n",
    "    acc.append(accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))\n",
    "plt.plot(list(range(1, 10)), acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [03:22<00:00, 33.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_accuracy = 0.8803294573643411 with loss ' log ', colsample = 1.0 , subsample = 0.67 , n_estimators = 5 and learning rate = 0.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_loss = \"\"\n",
    "best_lr = 0\n",
    "best_c = 0\n",
    "best_s = 0\n",
    "best_n = 0\n",
    "for alpha in tqdm(np.linspace(0.01, 0.8, 6)):\n",
    "    for n in range(2, 6):\n",
    "        for c in np.linspace(0.12, 1, 4):\n",
    "            for s in np.linspace(0.01, 1, 4):\n",
    "                for l in ['mse', 'exp', 'log']:\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "                    my_clf = MyGradientBoostingClassifier(loss = l, learning_rate=alpha, \n",
    "                                                          n_estimators=n, colsample=c, subsample=s)\n",
    "                    my_clf.fit(X_train, y_train)\n",
    "                    acc = accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test)\n",
    "                    if acc > best_acc:\n",
    "                        best_acc = acc\n",
    "                        best_loss = l\n",
    "                        best_lr = alpha\n",
    "                        best_c = c\n",
    "                        best_s = s\n",
    "                        best_n = n\n",
    "print(\"best_accuracy =\", best_acc, \"with loss '\", best_loss, \"', colsample =\", \n",
    "      best_c, \", subsample =\", best_s, \", n_estimators =\", best_n, \"and learning rate =\", best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BooBag BagBoo (1 балл)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем объединить бустинг и бэгинг. Давайте\n",
    "\n",
    "1) в качестве базовой модели брать не дерево решений, а случайный лес (из sklearn)\n",
    "\n",
    "2) обучать N бустингов на бустрапированной выборке, а затем предикт усреднять"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте обе этих стратегии на данных из прошлого задания. Получилось ли улучшить качество? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бустинг просто на деревьях: 0.8541666666666666\n",
      "бустинг на лесах из N деревьев с теми же параметрами бустинга: 0.8842054263565892\n"
     ]
    }
   ],
   "source": [
    "N = 20 # колво бустингов / колво деревьев в каждом лесе\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "my_clf = MyGradientBoostingClassifier(loss = best_loss, learning_rate=best_lr, \n",
    "                                        n_estimators=best_n, colsample=best_c, subsample=best_s)\n",
    "my_clf.fit(X_train, y_train, DecisionTreeRegressor)\n",
    "print(\"бустинг просто на деревьях:\", accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "my_clf = MyGradientBoostingClassifier(loss = best_loss, learning_rate=best_lr, \n",
    "                                        n_estimators=n, colsample=c, subsample=s)\n",
    "my_clf.fit(X_train, y_train, RandomForestRegressor, n_estimators=N)\n",
    "print(\"бустинг на лесах из N деревьев с теми же параметрами бустинга:\", accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "усреднение по N бустингам с теми же параметрами: 0.875\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "ans = np.zeros(len(y_test))\n",
    "for i in range(N):\n",
    "    indices = np.random.randint(0, len(y_train), (1, len(y_train)))[0]\n",
    "    X_b = X_train[indices, :]\n",
    "    y_b = y_train[indices]\n",
    "    my_clf = MyGradientBoostingClassifier(loss = best_loss, learning_rate=best_lr, \n",
    "                                        n_estimators=best_n, colsample=best_c, subsample=best_s)\n",
    "    my_clf.fit(X_b, y_b, DecisionTreeRegressor)\n",
    "    ans += my_clf.predict(X_test)\n",
    "ans = ans / N\n",
    "ans = np.round(ans)\n",
    "print(\"усреднение по N бустингам с теми же параметрами:\", accuracy_score(y_pred=ans, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Качество улучшилось в обоих способах. Видимо, variance был довольно велик \n",
    "# в бустинге из-за переобучения, и усреднение на каждом шаге \n",
    "# по N деревьям / усреднение N бустингов этот variance уменьшило"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Умная инициализация (1 балл)\n",
    "\n",
    "Попробуйте брать в качестве инициализации бустинга не константу, а какой-то алгоритм и уже от его предикта стартовать итерации бустинга. Попробуйте разные модели из sklearn: линейные модели, рандом форест, svm..\n",
    "\n",
    "Получилось ли улучшить качество? Почему?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init is Forest: 0.8880813953488372\n",
      "init is SVC: 0.8599806201550387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sasha\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init is LogisticRegression: 0.8544089147286822\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "my_clf = MyGradientBoostingClassifier(loss = best_loss, learning_rate=best_lr, \n",
    "                                    n_estimators=best_n, colsample=best_c, subsample=best_s)\n",
    "my_clf.fit(X_train, y_train, DecisionTreeRegressor, RandomForestClassifier)\n",
    "print(\"init is Forest:\", accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "my_clf = MyGradientBoostingClassifier(loss = best_loss, learning_rate=best_lr, \n",
    "                                    n_estimators=best_n, colsample=best_c, subsample=best_s)\n",
    "my_clf.fit(X_train, y_train, DecisionTreeRegressor, SVC)\n",
    "print(\"init is SVC:\", accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "my_clf = MyGradientBoostingClassifier(loss = best_loss, learning_rate=best_lr, \n",
    "                                    n_estimators=best_n, colsample=best_c, subsample=best_s)\n",
    "my_clf.fit(X_train, y_train, DecisionTreeRegressor, LogisticRegression)\n",
    "print(\"init is LogisticRegression:\", accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest с большим количеством деревьев в качестве init_model даёт прирост в точности,\n",
    "# так как лес сам по себе хорошо разделяет классы, а потом деревья доисправляют ошибки.\n",
    "\n",
    "# SVC и LogisticRegression дают незначительное изменение в точности. Я думаю, это связано с тем,\n",
    "# что данные не очень хорошо линейно разделимы, и при инициализации получается достаточная ошибка, \n",
    "# которую деревья не успевают полностью исправить за колво итераций n_estimators и шагом learning_rate, \n",
    "# которые подбирались для константной инициализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения  ансамблей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ваш ответ здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Здесь Вы можете оставить отзыв о этой домашней работе или о всем курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ВАШ ОТЗЫВ ЗДЕСЬ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
